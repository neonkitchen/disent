{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append('/home/neelan/dev/disent')\n",
    "from disent.model.ae import EncoderConv64, DecoderConv64\n",
    "encoder = EncoderConv64(x_shape=(3,64,64), z_size=9)\n",
    "decoder = DecoderConv64()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Flatten3D()\n",
       "  (9): Linear(in_features=1600, out_features=256, bias=True)\n",
       "  (10): ReLU(inplace=True)\n",
       "  (11): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(encoder.model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Flatten3D()\n",
       "  (9): Linear(in_features=1600, out_features=256, bias=True)\n",
       "  (10): ReLU(inplace=True)\n",
       "  (11): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=6, out_features=256, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): BatchView()\n",
       "  (5): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (10): ReLU(inplace=True)\n",
       "  (11): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/neelan/dev/disent')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from disent.data.groundtruth import XYSquaresClusterData\n",
    "from disent.dataset.groundtruth import GroundTruthDataset\n",
    "from disent.data.groundtruth.base import GroundTruthData\n",
    "from disent.transform import ToStandardisedTensor\n",
    "data: GroundTruthData = XYSquaresClusterData()\n",
    "dataset: Dataset = GroundTruthDataset(data, transform=ToStandardisedTensor())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=67, shuffle=False)\n",
    "images=next(iter(dataloader))['x_targ'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "images3ch = torch.hstack([images, images, images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 33, 33])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0](images3ch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### model.model(images3ch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([64, 3, 64, 64])\n",
      "Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "1\n",
      "torch.Size([64, 32, 33, 33])\n",
      "ReLU(inplace=True)\n",
      "2\n",
      "torch.Size([64, 32, 33, 33])\n",
      "Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "3\n",
      "torch.Size([64, 32, 17, 17])\n",
      "ReLU(inplace=True)\n",
      "4\n",
      "torch.Size([64, 32, 17, 17])\n",
      "Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "5\n",
      "torch.Size([64, 64, 9, 9])\n",
      "ReLU(inplace=True)\n",
      "6\n",
      "torch.Size([64, 64, 9, 9])\n",
      "Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "7\n",
      "torch.Size([64, 64, 5, 5])\n",
      "ReLU(inplace=True)\n",
      "8\n",
      "torch.Size([64, 64, 5, 5])\n",
      "Flatten3D()\n",
      "9\n",
      "torch.Size([64, 1600])\n",
      "Linear(in_features=1600, out_features=256, bias=True)\n",
      "10\n",
      "torch.Size([64, 256])\n",
      "ReLU(inplace=True)\n",
      "11\n",
      "torch.Size([64, 256])\n",
      "Linear(in_features=256, out_features=9, bias=True)\n",
      "torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print(i)\n",
    "    print(model.model[0:i](images3ch).shape)\n",
    "    print(encoder.model[i])\n",
    "print(model.model(images3ch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderConv64(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): BatchView()\n",
       "    (5): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderConv64(x_shape=(3,64,64), z_size=9)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand((64,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([64, 9])\n",
      "Linear(in_features=9, out_features=256, bias=True)\n",
      "1\n",
      "torch.Size([64, 256])\n",
      "ReLU(inplace=True)\n",
      "2\n",
      "torch.Size([64, 256])\n",
      "Linear(in_features=256, out_features=1024, bias=True)\n",
      "3\n",
      "torch.Size([64, 1024])\n",
      "ReLU(inplace=True)\n",
      "4\n",
      "torch.Size([64, 1024])\n",
      "BatchView()\n",
      "5\n",
      "torch.Size([64, 64, 4, 4])\n",
      "ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "6\n",
      "torch.Size([64, 64, 8, 8])\n",
      "ReLU(inplace=True)\n",
      "7\n",
      "torch.Size([64, 64, 8, 8])\n",
      "ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "8\n",
      "torch.Size([64, 32, 16, 16])\n",
      "ReLU(inplace=True)\n",
      "9\n",
      "torch.Size([64, 32, 16, 16])\n",
      "ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "10\n",
      "torch.Size([64, 32, 32, 32])\n",
      "ReLU(inplace=True)\n",
      "11\n",
      "torch.Size([64, 32, 32, 32])\n",
      "ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "torch.Size([64, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print(i)\n",
    "    print(decoder.model[0:i](z).shape)\n",
    "    print(decoder.model[i])\n",
    "print(decoder.model(z).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch.onnx\n",
    "\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "torch_model = encoder.model\n",
    "\n",
    "from torch.autograd import Variable\n",
    "batch_size = 64    # just a random number\n",
    "\n",
    "# Input to the model\n",
    "x = Variable(torch.randn(images3ch.shape), requires_grad=True)\n",
    "\n",
    "# Export the model\n",
    "torch_out = torch.onnx._export(torch_model,             # model being run\n",
    "                               x,                       # model input (or a tuple for multiple inputs)\n",
    "                               \"squeezenet.onnx\",       # where to save the model (can be a file or file-like object)\n",
    "                               export_params=True)      # store the trained parameter weights inside the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.18.2-py3-none-any.whl (39 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.18.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy protobuf==3.16.0\n",
    "#!pip install onnx\n",
    "#!pip install pydot\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neelan/dev/env/miniconda3/envs/disent/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: onnx\r\n",
      "Version: 1.10.2\r\n",
      "Summary: Open Neural Network Exchange\r\n",
      "Home-page: https://github.com/onnx/onnx\r\n",
      "Author: ONNX\r\n",
      "Author-email: onnx-technical-discuss@lists.lfai.foundation\r\n",
      "License: Apache License v2.0\r\n",
      "Location: /home/neelan/dev/env/miniconda3/envs/disent/lib/python3.8/site-packages\r\n",
      "Requires: six, numpy, typing-extensions, protobuf\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MscRR_divergences_vs_modes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
