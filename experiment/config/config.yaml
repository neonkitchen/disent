defaults:
  # experiment
  - framework: betagammavae
  - model: conv64
  - optimizer: radam
  - dataset: xysquares  # allow framework to override settings here, but placing dataset before framework in defaults
  - augment: none
  - sampling: full_bb
  - metrics: all
  - schedule: none
  # runtime
  - run_length: tiny
  - run_location: cluster
  - run_callbacks: vis_slow
  - run_logging: none
  # plugins
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  - hydra/launcher: submitit_slurm

job:
  user: 'n_michlo'
  project: 'DELETE'
  name: '${framework.name}:${framework.module.recon_loss}|${dataset.name}:${sampling.name}|${trainer.steps}'
  partition: stampede

framework:
    beta: 0.002
    gamma: 0.002
    module:
      recon_loss: mse4
      loss_reduction: mean
      latent_distribution: cauchy
      kl_loss_mode: direct_fdiv
    optional:
      latent_distribution: normal  # only used by VAEs
      overlap_loss: NULL

model:
  z_size: 9

optimizer:
  lr: 5e-4

# CUSTOM DEFAULTS SPECIALIZATION
# - This key is deleted on load and the correct key on the root config is set similar to defaults.
# - Unfortunately this hack needs to exists as hydra does not yet support this kinda of variable interpolation in defaults.
specializations:
  data_wrapper: ${dataset.data_type}_${framework.data_wrap_mode}
