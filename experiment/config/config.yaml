defaults:
  # experiment
  - framework: betagammavae
  - model: conv64
  - optimizer: radam
  - dataset: dsprites # allow framework to override settings here, but placing dataset before framework in defaults
  - augment: none
  - sampling: full_bb
  - metrics: test
  - schedule: none
  # runtime
  - run_length: medium
  - run_location: cluster_many
  - run_callbacks: test
  - run_logging: wandb
  # plugins
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  - hydra/launcher: submitit_slurm

job:
  user: 'neonkitchen'
  project: 'disent_sweep_batch'
  name: '${dataset.name}:${framework.beta}:${framework.gamma}:${framework.name}:${framework.module.recon_loss}|${dataset.name}:${sampling.name}|${trainer.steps}'
  partition: stampede

framework:
    beta: 0.001
    gamma: 0.001
    module:
      recon_loss: mse4
      loss_reduction: mean
      kl_loss_mode: direct_fdiv
    optional:
      latent_distribution: normal  # only used by VAEs
      overlap_loss: NULL

model:
  z_size: 9

optimizer:
  lr: 5e-4

# CUSTOM DEFAULTS SPECIALIZATION
# - This key is deleted on load and the correct key on the root config is set similar to defaults.
# - Unfortunately this hack needs to exists as hydra does not yet support this kinda of variable interpolation in defaults.
specializations:
  data_wrapper: ${dataset.data_type}_${framework.data_wrap_mode}
