{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from numbers import Number\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import final\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Laplace\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from disent.frameworks.ae._unsupervised__ae import Ae\n",
    "from disent.frameworks.vae import Vae\n",
    "from disent.frameworks.vae import BetaVae\n",
    "\n",
    "from disent.frameworks.helper.latent_distributions import LatentDistsHandler\n",
    "from disent.frameworks.helper.latent_distributions import make_latent_distribution\n",
    "from disent.frameworks.helper.util import detach_all\n",
    "\n",
    "from disent.util import map_all\n",
    "\n",
    "from dataclasses import fields\n",
    "from typing import Sequence\n",
    "from typing import Tuple, final\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from disent.frameworks.helper.reductions import loss_reduction\n",
    "from disent.frameworks.helper.util import compute_ave_loss\n",
    "from disent.frameworks.helper.latent_distributions import LatentDistsHandler\n",
    "\n",
    "\n",
    "from disent.model.ae.base import AutoEncoder\n",
    "REQUIRED_Z_MULTIPLIER = 2\n",
    "REQUIRED_OBS = 1\n",
    "\n",
    "\n",
    "#_model: AutoEncoder = make_model_fn() \n",
    "# --------------------------------------------------------------------- #\n",
    "# VAE Training Step                                                     #\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "def _get_xs_and_targs(batch: Dict[str, Tuple[torch.Tensor, ...]]) -> Tuple[Tuple[torch.Tensor, ...], Tuple[torch.Tensor, ...]]:\n",
    "    xs_targ = batch['x_targ']\n",
    "    if 'x' not in batch:\n",
    "        warnings.warn('dataset does not have input: x -> x_targ using target as input: x_targ -> x_targ')\n",
    "        xs = xs_targ\n",
    "    else:\n",
    "        xs = batch['x']\n",
    "    # check that we have the correct number of inputs\n",
    "    if (len(xs) != REQUIRED_OBS) or (len(xs_targ) != REQUIRED_OBS):\n",
    "        log.warning(f'batch len(xs)={len(xs)} and len(xs_targ)={len(xs_targ)} observation count mismatch, requires: {REQUIRED_OBS}')\n",
    "    # done\n",
    "    return xs, xs_targ\n",
    "\n",
    "def do_training_step(batch, batch_idx):\n",
    "    xs, xs_targ = _get_xs_and_targs(batch, batch_idx)\n",
    "\n",
    "    # FORWARD\n",
    "    # -~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~- #\n",
    "    # latent distribution parameterizations\n",
    "    ds_posterior, ds_prior = map_all(encode_dists, xs, collect_returned=True)\n",
    "\n",
    "    \"\"\"\n",
    "    # [HOOK] disable learnt scale values\n",
    "    ds_posterior, ds_prior = _hook_intercept_ds_disable_scale(ds_posterior, ds_prior)\n",
    "    # [HOOK] intercept latent parameterizations\n",
    "    ds_posterior, ds_prior, logs_intercept_ds = hook_intercept_ds(ds_posterior, ds_prior)\n",
    "    \"\"\"\n",
    "    # sample from dists\n",
    "    zs_sampled = tuple(d.rsample() for d in ds_posterior)\n",
    "    # reconstruct without the final activation\n",
    "    xs_partial_recon = map_all(decode_partial, detach_all(zs_sampled, if_=cfg.disable_decoder))\n",
    "    # -~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~- #\n",
    "\n",
    "    # LOSS\n",
    "    # -~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~- #\n",
    "    # compute all the recon losses\n",
    "    recon_loss, logs_recon = compute_ave_recon_loss(xs_partial_recon, xs_targ)\n",
    "    # compute all the regularization losses\n",
    "    reg_loss, logs_reg = compute_ave_reg_loss(ds_posterior, ds_prior, zs_sampled)\n",
    "    # [HOOK] augment loss\n",
    "    aug_loss, logs_aug = hook_compute_ave_aug_loss(ds_posterior=ds_posterior, ds_prior=ds_prior, zs_sampled=zs_sampled, xs_partial_recon=xs_partial_recon, xs_targ=xs_targ)\n",
    "    # compute combined loss\n",
    "    loss = 0\n",
    "    if not cfg.disable_rec_loss: loss += recon_loss\n",
    "    if not cfg.disable_aug_loss: loss += aug_loss\n",
    "    if not cfg.disable_reg_loss: loss += reg_loss\n",
    "    # -~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~- #\n",
    "\n",
    "    # return values\n",
    "    return loss, {\n",
    "        **logs_intercept_ds,\n",
    "        **logs_recon,\n",
    "        **logs_reg,\n",
    "        **logs_aug,\n",
    "        'recon_loss': recon_loss,\n",
    "        'reg_loss': reg_loss,\n",
    "        'aug_loss': aug_loss,\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# Delete AE Hooks                                                       #\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def hook_ae_intercept_zs(zs: Sequence[torch.Tensor]) -> Tuple[Sequence[torch.Tensor], Dict[str, Any]]:\n",
    "    raise NotImplementedError('This function should never be used or overridden by VAE methods!')  # pragma: no cover\n",
    "\n",
    "\n",
    "def hook_ae_compute_ave_aug_loss(zs: Sequence[torch.Tensor], xs_partial_recon: Sequence[torch.Tensor], xs_targ: Sequence[torch.Tensor]) -> Tuple[Union[torch.Tensor, Number], Dict[str, Any]]:\n",
    "    raise NotImplementedError('This function should never be used or overridden by VAE methods!')  # pragma: no cover\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# Private Hooks                                                         #\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "def _hook_intercept_ds_disable_scale(ds_posterior: Sequence[Distribution], ds_prior: Sequence[Distribution]):\n",
    "    # disable posterior scales\n",
    "    if cfg.disable_posterior_scale is not None:\n",
    "        for d_posterior in ds_posterior:\n",
    "            assert isinstance(d_posterior, (Normal, Laplace))\n",
    "            d_posterior.scale = torch.full_like(d_posterior.scale, fill_value=cfg.disable_posterior_scale)\n",
    "    # return modified values\n",
    "    return ds_posterior, ds_prior\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# Overrideable Hooks                                                    #\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "def hook_intercept_ds(ds_posterior: Sequence[Distribution], ds_prior: Sequence[Distribution]) -> Tuple[Sequence[Distribution], Sequence[Distribution], Dict[str, Any]]:\n",
    "    return ds_posterior, ds_prior, {}\n",
    "\n",
    "def hook_compute_ave_aug_loss(ds_posterior: Sequence[Distribution], ds_prior: Sequence[Distribution], zs_sampled: Sequence[torch.Tensor], xs_partial_recon: Sequence[torch.Tensor], xs_targ: Sequence[torch.Tensor]) -> Tuple[Union[torch.Tensor, Number], Dict[str, Any]]:\n",
    "    return 0, {}\n",
    "\n",
    "def compute_ave_reg_loss(ds_posterior: Sequence[Distribution], ds_prior: Sequence[Distribution], zs_sampled: Sequence[torch.Tensor]) -> Tuple[Union[torch.Tensor, Number], Dict[str, Any]]:\n",
    "    # compute regularization loss (kl divergence)\n",
    "    kl_loss = latents_handler.compute_ave_kl_loss(ds_posterior, ds_prior, zs_sampled)\n",
    "    # return logs\n",
    "    return kl_loss, {\n",
    "        'kl_loss': kl_loss,\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# VAE - Encoding - Overrides AE                                         #\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "def encode(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Get the deterministic latent representation (useful for visualisation)\"\"\"\n",
    "    z_raw = module.encode(x)\n",
    "    z = latents_handler.encoding_to_representation(z_raw)\n",
    "    return z\n",
    "\n",
    "cfg=BetaVae.cfg(beta=0.003, loss_reduction='mean')\n",
    "latents_handler = make_latent_distribution(cfg.latent_distribution, kl_mode=cfg.kl_loss_mode, reduction=cfg.loss_reduction)\n",
    "\n",
    "\n",
    "def encode_dists(x: torch.Tensor) -> Tuple[Distribution, Distribution]:\n",
    "    \"\"\"Get parametrisations of the latent distributions, which are sampled from during training.\"\"\"\n",
    "    z_raw = module.encode(x)\n",
    "    z_posterior, z_prior = latents_handler.encoding_to_dists(z_raw)\n",
    "    return z_posterior, z_prior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Normal(loc: torch.Size([32, 6]), scale: torch.Size([32, 6])),\n",
       " Normal(loc: torch.Size([32, 6]), scale: torch.Size([32, 6])))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from disent.dataset.groundtruth import GroundTruthDatasetTriples\n",
    "from disent.dataset.groundtruth import GroundTruthDistDataset\n",
    "from disent.metrics._flatness import get_device\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.data.groundtruth import XYObjectData, XYSquaresData\n",
    "from disent.frameworks.vae import BetaVae\n",
    "from disent.model.ae import EncoderConv64, DecoderConv64, AutoEncoder\n",
    "from disent.transform import ToStandardisedTensor\n",
    "from disent.util import colors\n",
    "from disent.util import Timer\n",
    "\n",
    "def get_str(r):\n",
    "    return ', '.join(f'{k}={v:6.4f}' for k, v in r.items())\n",
    "\n",
    "def print_r(name, steps, result, clr=colors.lYLW, t: Timer = None):\n",
    "    print(f'{clr}{name:<13} ({steps:>04}){f\" {colors.GRY}[{t.pretty}]{clr}\" if t else \"\"}: {get_str(result)}{colors.RST}')\n",
    "\n",
    "def calculate(name, steps, dataset, get_repr):\n",
    "    print(get_repr)\n",
    "    #global aggregate_measure_distances_along_factor\n",
    "    #with Timer() as t:\n",
    "    #    r = {\n",
    "    #    #**metric_flatness_components(dataset, get_repr, factor_repeats=64, batch_size=64),\n",
    "    #    #    **metric_flatness(dataset, get_repr, factor_repeats=64, batch_size=64),\n",
    "    #    }\n",
    "    #results.append((name, steps, r))\n",
    "    #print_r(name, steps, r, colors.lRED, t=t)\n",
    "    #print(colors.GRY, '='*100, colors.RST, sep='')\n",
    "    #return r\n",
    "    \n",
    "\n",
    "class XYOverlapData(XYSquaresData):\n",
    "    def __init__(self, square_size=8, grid_size=64, grid_spacing=None, num_squares=3, rgb=True):\n",
    "        if grid_spacing is None:\n",
    "            grid_spacing = (square_size+1) // 2\n",
    "        super().__init__(square_size=square_size, grid_size=grid_size, grid_spacing=grid_spacing, num_squares=num_squares, rgb=rgb)\n",
    "\n",
    "####################\n",
    "#### train. ########\n",
    "####################\n",
    "\n",
    "results=[]\n",
    "data= XYSquaresData()\n",
    "dataset = GroundTruthDistDataset(data, transform=ToStandardisedTensor())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "cfg=BetaVae.cfg(beta=0.003, loss_reduction='mean')\n",
    "module = BetaVae(\n",
    "    make_optimizer_fn=lambda params: Adam(params, lr=5e-4),\n",
    "    make_model_fn=lambda: AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=cfg\n",
    ")\n",
    "\n",
    "latents_handler = make_latent_distribution(cfg.latent_distribution, kl_mode=cfg.kl_loss_mode, reduction=cfg.loss_reduction)\n",
    "\n",
    "datum = next(iter(dataloader))\n",
    "[x] = datum['x_targ']\n",
    "z_posterior, z_prior = module.encode_dists(x)\n",
    "z_posterior, z_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6505, -0.9901,  0.4339,  0.5095,  0.8773, -1.7774],\n",
       "        [ 0.4489,  1.8013,  0.7695, -0.3043,  0.4200,  1.4606],\n",
       "        [ 1.9153,  1.0503, -2.0272,  0.6760,  1.3001, -1.1350],\n",
       "        [ 0.7543,  0.3628,  1.0625, -1.5774,  0.1372, -0.9330],\n",
       "        [ 0.0245, -0.0343, -0.5464, -1.5049, -1.2338,  0.0360],\n",
       "        [-0.0178,  0.0234, -0.2680,  0.0583,  0.2498,  1.1666],\n",
       "        [-0.6733, -2.5286,  1.2362, -0.7957, -0.1021,  0.3970],\n",
       "        [-0.0639,  0.6298,  1.4576,  0.1052, -0.3022, -0.3363],\n",
       "        [ 1.4955, -0.0432, -2.7151, -0.5484,  0.1370,  0.2457],\n",
       "        [-0.8257, -1.3647, -0.9488, -0.4407, -0.2710,  0.6412],\n",
       "        [-1.2137,  0.3191,  0.1291, -0.8512,  0.7040,  1.0466],\n",
       "        [-0.6759,  0.5157,  1.7835,  0.0582,  0.2333, -0.6896],\n",
       "        [-1.0725,  1.8992,  0.5191, -0.9337,  1.7056,  0.6230],\n",
       "        [ 1.5748,  2.1236,  0.2903,  0.5397, -0.8111,  0.3532],\n",
       "        [ 0.5568,  0.4835, -1.0068, -1.3208,  0.6463, -1.5340],\n",
       "        [ 0.2054,  0.3039, -0.0127, -0.3372, -2.3198,  1.5570],\n",
       "        [-0.8968,  0.7644,  0.9750,  0.3377,  0.3610, -0.8054],\n",
       "        [-0.2885,  0.3444, -0.0539,  0.1170, -0.4803, -1.1650],\n",
       "        [-0.0751,  2.3333, -1.0324, -0.4104, -0.4335, -1.1114],\n",
       "        [-0.6876,  0.0445, -0.3145, -1.1243, -0.1307, -2.1071],\n",
       "        [-2.0287,  0.5815,  0.1042, -0.7589,  1.8194, -0.8802],\n",
       "        [ 0.1272,  0.5871, -0.2994, -0.6418, -0.6490, -1.3374],\n",
       "        [ 0.4003, -0.8083, -1.1099, -1.1775,  0.5037,  0.7022],\n",
       "        [-2.2523, -0.1849,  0.1503,  2.0652,  1.5395,  0.1939],\n",
       "        [-0.6353,  0.4820,  1.1578, -0.6177,  0.2588,  0.2203],\n",
       "        [-1.0377, -1.5138,  0.3276,  0.0277, -0.1301,  0.1202],\n",
       "        [ 0.2147,  1.5068, -0.1064,  1.5727, -1.9486,  0.1350],\n",
       "        [ 0.8244,  0.5075, -2.8060,  0.4773, -1.6660, -0.8677],\n",
       "        [-1.0893, -0.2097,  0.0051,  0.4683, -0.4234,  0.0881],\n",
       "        [-0.2288, -2.3869, -1.7160, -0.4829,  0.1810, -2.3072],\n",
       "        [-1.7116, -0.6738,  0.4723,  0.7737, -0.7340,  0.2676],\n",
       "        [-0.0869,  1.3021,  0.8163, -0.4049, -0.0307,  1.2856]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_prior.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('distent')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23963c1f86699c61d89cadad3a7ec9c75b0d41bba394437f90a75fd6cfbe6199"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
